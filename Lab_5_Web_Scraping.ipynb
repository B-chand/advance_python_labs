{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65cc963c",
   "metadata": {},
   "source": [
    "# LAB 5: Web Scraping using Python\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d28e9b",
   "metadata": {},
   "source": [
    "## Question 1: Basic HTML Request and Parsing\n",
    "Write a Python program to fetch the HTML content of https://www.geeksforgeeks.org using requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.geeksforgeeks.org'\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c8d185",
   "metadata": {},
   "source": [
    "## Question 2: Parse HTML and Print Page Title\n",
    "Parse the HTML using BeautifulSoup and print the title of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06355d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3138eb",
   "metadata": {},
   "source": [
    "## Question 3: Handle HTTP and Network Errors\n",
    "Handle HTTP errors and network exceptions while fetching the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401bd411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import HTTPError, ConnectionError, Timeout, RequestException\n",
    "\n",
    "try:\n",
    "    r = requests.get(url, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    print('Success')\n",
    "except HTTPError as e:\n",
    "    print('HTTP Error', e)\n",
    "except ConnectionError:\n",
    "    print('Connection Error')\n",
    "except Timeout:\n",
    "    print('Timeout Error')\n",
    "except RequestException as e:\n",
    "    print('Request Error', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2ce8c",
   "metadata": {},
   "source": [
    "## Question 4: Extract Hyperlinks\n",
    "Extract the first five hyperlinks using find() and find_all()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e269ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a', limit=5)\n",
    "for link in links:\n",
    "    print(link.get_text(strip=True), link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76cf083",
   "metadata": {},
   "source": [
    "## Question 5: Extract H2 Headings and Save to CSV\n",
    "Scrape all <h2> headings and store them in headings.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c07103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "headings = soup.find_all('h2')\n",
    "\n",
    "with open('headings.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Heading Number', 'H2 Text'])\n",
    "    for i, h in enumerate(headings, 1):\n",
    "        writer.writerow([i, h.get_text(strip=True)])\n",
    "\n",
    "print('headings.csv created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25fecdb",
   "metadata": {},
   "source": [
    "## Question 6: Scrape Wikipedia Table\n",
    "Scrape all rows from the first table of Wikipedia page: List of countries by population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_url = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\n",
    "wiki_res = requests.get(wiki_url)\n",
    "wiki_soup = BeautifulSoup(wiki_res.text, 'html.parser')\n",
    "\n",
    "table = wiki_soup.find('table', class_='wikitable')\n",
    "for row in table.find_all('tr'):\n",
    "    cells = row.find_all(['th', 'td'])\n",
    "    data = [cell.get_text(strip=True) for cell in cells]\n",
    "    if data:\n",
    "        print(data)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}